{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b94bd43-3190-4026-ab5b-b873c8f13a30",
     "showTitle": true,
     "title": "Data Quality Checks for Financial Transactions"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "py4j.Py4JException: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\n  File \"/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 642, in _call_proxy\n    return_value = getattr(self.pool[obj_id], method)(*params)\n  File \"/databricks/spark/python/dlt/helpers.py\", line 31, in call\n    res = self.func()\n  File \"/root/.ipykernel/1450/command-2020870890715496-3359285086\", line 55, in source_data_validated\n    return dlt.read(\"source_2\")\n  File \"/databricks/spark/python/dlt/api.py\", line 598, in read\n    pipeline.instance.get_scala_pipeline().read(name),\n  File \"/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1355, in __call__\n    return_value = get_return_value(\n  File \"/databricks/spark/python/pyspark/errors/exceptions/captured.py\", line 230, in deco\n    raise converted from None\npyspark.errors.exceptions.captured.AnalysisException: Failed to read dataset 'source_2'. Dataset is not defined in the pipeline.\n\n"
     ]
    }
   ],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import col, lit, count\n",
    "from pyspark.sql.types import IntegerType, FloatType, StringType, DateType\n",
    "\n",
    "#FinalTable\n",
    "@dlt.table(\n",
    "    name=\"FinancialTransactions_Checks\",\n",
    "    comment=\"Final table with data quality checks.\"\n",
    ")\n",
    "@dlt.expect(\"valid_client_number\", \"ClientNumber IS NOT NULL AND ClientNumber > 0\")\n",
    "@dlt.expect(\"valid_original_amount\", \"OriginalAmount IS NOT NULL AND OriginalAmount >= 0\")\n",
    "@dlt.expect(\"valid_onboarding_date\", \"OnboardingDate IS NOT NULL\")\n",
    "@dlt.expect(\"valid_amount_eur\", \"AmountEUR IS NOT NULL AND AmountEUR >= 0\")\n",
    "@dlt.expect(\"check_client_number_type\", \"CAST(ClientNumber AS INTEGER) IS NOT NULL\")\n",
    "@dlt.expect(\"check_amount_eur_type\", \"CAST(AmountEUR AS FLOAT) IS NOT NULL\")\n",
    "@dlt.expect(\"enterprise_size_range\", \"EnterpriseSize IN ('S', 'M', 'L')\")\n",
    "@dlt.expect(\"secured_amount_range\", \"SecuredAmountEUR >= 0 AND SecuredAmountEUR <= 500000\")\n",
    "def read_final_table():\n",
    "    return spark.read.table(\"hive_metastore.default.financialtransactions\")\n",
    "\n",
    "# Source1\n",
    "@dlt.table(\n",
    "    name=\"Source_1_Checks\",\n",
    "    comment=\"Final table with data quality checks.\"\n",
    ")\n",
    "@dlt.expect(\"non_null_DataSource\", \"DataSource IS NOT NULL\")\n",
    "@dlt.expect(\"non_null_ClientGroup\", \"ClientGroup IS NOT NULL\")\n",
    "@dlt.expect(\"non_null_ClientNumber\", \"ClientNumber IS NOT NULL\")\n",
    "@dlt.expect(\"valid_ClientAmount\", \"ClientAmount >= 0\")\n",
    "@dlt.expect(\"non_null_Currency\", \"Currency IS NOT NULL\")\n",
    "@dlt.expect(\"valid_NumberOfEmployees\", \"NumberOfEmployees >= 0\")\n",
    "@dlt.expect(\"non_null_Location\", \"Location IS NOT NULL\")\n",
    "@dlt.expect(\"valid_ClientSince_format\", \"ClientSince RLIKE '^[0-9]{4}-[0-9]{2}-[0-9]{2}$'\")\n",
    "@dlt.expect(\"non_null_EligibleForDiscount\", \"EligibleForDiscount IS NOT NULL\")\n",
    "@dlt.expect(\"valid_SnapshotDate_format\", \"SnapshotDate RLIKE '^[0-9]{4}-[0-9]{2}-[0-9]{2}$'\")\n",
    "def final_table():\n",
    "    df = spark.read.table(\"source_1\")\n",
    "    return df\n",
    "\n",
    "# Source2\n",
    "@dlt.table(\n",
    "    name=\"Source_2_Checks\",\n",
    "    comment=\"Validated data from the source.\"\n",
    ")\n",
    "@dlt.expect(\"valid_clientsince\", \"ClientSince RLIKE '^[0-9]{4}-[0-9]{2}-[0-9]{2}$'\")\n",
    "@dlt.expect(\"valid_snapshotdate\", \"SnapshotDate RLIKE '^[0-9]{4}-[0-9]{2}-[0-9]{2}$'\")\n",
    "@dlt.expect(\"non_null_clientsince\", \"ClientSince IS NOT NULL\")\n",
    "@dlt.expect(\"non_null_snapshotdate\", \"SnapshotDate IS NOT NULL\")\n",
    "@dlt.expect(\"valid_hasloan\", \"HasLoan IN ('Yes', 'No')\")\n",
    "@dlt.expect(\"positive_clientamount\", \"ClientAmount > 0\")\n",
    "@dlt.expect(\"non_null_clientsystem\", \"SourceSystem IS NOT NULL\")\n",
    "@dlt.expect(\"positive_numberofemployees\", \"NumberOfEmployees > 0\")\n",
    "@dlt.expect(\"non_negative_clientnumber\", \"ClientNumber IS NOT NULL\")\n",
    "def source_data_validated():\n",
    "    return dlt.read(\"source_2\")\n",
    "\n",
    "#Exchenge_Rates\n",
    "@dlt.table(\n",
    "    name=\"Exchange_Rates_Checks\",\n",
    "    comment=\"Exchange_Rates Table with data quality checks.\"\n",
    ")\n",
    "@dlt.expect_or_fail(\"non_null_currency\", \"Currency IS NOT NULL\")\n",
    "@dlt.expect_or_fail(\"valid_exchange_rate\", \"ExchangeRate > 0\")\n",
    "@dlt.expect_or_fail(\"non_null_snapshot_date\", \"SnapshotDate IS NOT NULL\")\n",
    "def exchange_rates_validated():\n",
    "    return spark.read.table(\"hive_metastore.default.exchange_rates\") # check the correct name\n",
    "\n",
    "#Client_Secured\n",
    "@dlt.table(\n",
    "    name=\"Client_Secured_Ind_checks\",\n",
    "    comment=\"Table with data quality checks for Client_Secured_Ind.\"\n",
    ")\n",
    "@dlt.expect(\"valid_client_secured_ind\", \"ClientSecuredInd IN ('Y', 'N')\")\n",
    "@dlt.expect(\"non_null_client_number\", \"ClientNumber IS NOT NULL\")\n",
    "def client_secured_ind_validated():\n",
    "    return spark.read.table(\"hive_metastore.default.client_secured_ind\")\n",
    "\n",
    "@dlt.view(\n",
    "    name=\"Client_Secured_Ind_with_duplicates\",\n",
    "    comment=\"View to check for duplicates in Client_Secured_Ind.\"\n",
    ")\n",
    "def client_secured_ind_with_duplicates():\n",
    "    df = dlt.read(\"Client_Secured_Ind_checks\")\n",
    "    duplicate_count = df.groupBy(\"ClientNumber\").agg(count(\"*\").alias(\"count\"))\n",
    "    duplicates = duplicate_count.filter(col(\"count\") > 1)\n",
    "    return df.join(duplicates, \"ClientNumber\", \"left_outer\").withColumn(\"is_duplicate\", col(\"count\").isNotNull())\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"Final_Client_Secured_Ind\",\n",
    "    comment=\"Final table for Client_Secured_Ind with all validations.\"\n",
    ")\n",
    "@dlt.expect(\"no_duplicates\", \"is_duplicate = false\")\n",
    "def final_client_secured_ind():\n",
    "    return dlt.read(\"Client_Secured_Ind_with_duplicates\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2020870890715575,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Data Quality Check",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
